{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=7 color=\"#2E1698\"><u><center>Twitter French Analysis</center></u></font></p><br>\n",
    "<p><font size=3.2 color=\"#2E1698\"><i><u>Introduction:</u></i> <br>We are here aiming to manipulate the data that we generated before. Our data was provided by the Twitter API and was filtring with the '#PrimaireLeDébat' hashtag. We recovered our data Thursday 19th January 2017 after the third french socialist political debat before presidential elections. Today we know Benoit Hamon won the elections whereas Manuel Valls was second. The other participants was Arnaud Montebourg, Vincent Peillon, Sylvia Pinel, François de Rugy and Jean-Luc Bennahmias. This debat was broadcast on the TV Channel 'France 2' and on the radio 'Europe 1'.</font><p>\n",
    "<br>\n",
    "<font color=\"#206B50\" size = 4.5><center>**SUMMARY**</center></font>\n",
    "<font size=3.2 color=\"#2E1698\">\n",
    "<b>Part I:</b> \n",
    "      <ol>\n",
    "          <li>Data visualization</li>\n",
    "          <li>Data cleaning</li>\n",
    "      </ol>\n",
    "<br>\n",
    "<b>Part II:</b>\n",
    "      <ol>\n",
    "          <li>Provide a list of the 15 most common words</li>\n",
    "          <li>Provide a list of the 2 pairs of words having the highest co-occurrence frequency</li>\n",
    "      </ol>\n",
    "<br>\n",
    "<b>Part III:</b>\n",
    "      <ol>\n",
    "          <li>Build a graphical representation of the most frequent words with their polarity (pos/neg or anger/joy/fear/...)</li>\n",
    "          <li>Indicate the 3 most frequent representatives words in each category</li>\n",
    "          <li>Compare the results of the two approaches</li>\n",
    "      </ol>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<p><font size=4.5 color=\"#206B50\"><center><B>PART I - 1. DATA VISUALIZATION</B></center></font></p>\n",
    "<font size=3 color=\"#2E1698\">First things first, let's import the csv file. We are using french tweets this is why we need to specify the UTF-8 encoding. To have a better idea of our database we decided to show the first 5 lines.</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Please uncomment the following lines to download the packages that are necessary for this notebook.\n",
    "\n",
    "#install.packages(\"tm\") #need R 3.3.2\n",
    "#install.packages(\"wordcloud\")\n",
    "#install.packages(\"SnowballC\")\n",
    "#install.packages(\"plyr\")\n",
    "#install.packages(\"devtools\")\n",
    "#install.packages('ggplot2')\n",
    "#require(\"devtools\")\n",
    "#install_github(\"lchiffon/wordcloud2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yourPath = paste0(getwd(),\"/data/debat_primaire_20000.csv\")\n",
    "tweets = read.csv(yourPath, encoding=\"UTF-8\")\n",
    "print(dim(tweets))\n",
    "head(tweets, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font color=\"#2E1698\" size = 3.2>As you can see, our data frame contains <font color=\"red\">17</font> columns and <font color=\"red\">20 000</font> rows, let's see the 10 firsts rows.</font><br></p>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see if all the columns have multiple values, or if some are useless.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"UNIQUE\", \"\\n\")\n",
    "cat(\"Favorited: \",head(unique(tweets$favorited)), \"\\n\")\n",
    "cat(\"FavoriteCount: \",head(unique(tweets$favoriteCount)), \"\\n\")\n",
    "cat(\"ReplyToSN: \",head(unique(tweets$replyToSN)), \"\\n\")\n",
    "cat(\"ReplyToUID: \",head(unique(tweets$replyToUID)), \"\\n\")\n",
    "cat(\"Id: \",head(unique(tweets$id)), \"\\n\")\n",
    "cat(\"IsRetweet: \",head(unique(tweets$isRetweet)), \"\\n\")\n",
    "cat(\"Lattitude: \",head(unique(tweets$latitude)), \"\\n\")\n",
    "cat(\"Longitude: \",head(unique(tweets$longitude)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Favorited TRUE: \", length(which(tweets$favorited == \"TRUE\")), \"\\n\")\n",
    "cat(\"Favorited FALSE: \", length(which(tweets$favorited == \"FALSE\")), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font  \n",
    "color=\"#2E1698\" size = 3.2>We can see here that there is no TRUE value for favorited, only FALSE. favorited is useless though.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"No favoritedCount: \", length(which(tweets$favoriteCount == 0)), \"\\n\")\n",
    "cat(\"At least one favoritedCount: \", length(which(tweets$favoriteCount != 0)), \"\\n\")\n",
    "cat(\"Percentage of tweets that have favoritedCount\",(3919/20000)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>The favoriteCount have multiple values, 20% of the are not 0 we better keep this column. It is maybe a significative data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Tweets containing longitude information: \", length(which(tweets$longitude != \"NA\")), \"\\n\")\n",
    "cat(\"Tweets containing latitude information: \", length(which(tweets$latitude != \"NA\")), \"\\n\")\n",
    "cat(\"Percentage of tweets containing infos: \", 9/20000*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>There is only 9 tweets over 20 000 that contains latitude and longitude, this represents only 0.045% of the tweets, this info can be considered as useless, and we can delete this two columns too.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Tweets with replyToSN not null\", length(which(tweets$replyToSN != \"NA\")),\"\\n\")\n",
    "cat(\"Tweets with replyToUID not null\", length(which(tweets$replyToUID != 'NA')), \"\\n\")\n",
    "cat (\"Tweets with replyToSID not null\", length(which(tweets$replyToSID != 'NA')), \"\\n\")\n",
    "cat(\"Percentage of tweets containing replyToSN/UID\", 698/20000*100, \"%\", \"\\n\")\n",
    "cat(\"Percentage of tweets containing replyToSID\", 445/20000*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>There is only about 3.5% of the replytoSN and replyToUID data that are not NA, we can delete these two columns as they don't seem to be interesting to study. Same thing for the replyToSID, with less than 2.5%.</font> <br><br><br>\n",
    "<font size=4.5 color=\"#206B50\"><center><B>PART I - 2. DATA CLEANING</B></center></font>\n",
    "<br>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's delete these useless columns!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets <- subset(tweets, select=-c(replyToSN,replyToUID, replyToSID, latitude, longitude, favorited))\n",
    "head(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>If we want to use the text, it have to be cleaned first</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_text = function(x)\n",
    "{\n",
    "    #To convert the text in lowercase\n",
    "    try.error = function(z)\n",
    "    {\n",
    "        y = NA\n",
    "        try_error = tryCatch(tolower(z), error=function(e) e)\n",
    "            if (!inherits(try_error, \"error\"))\n",
    "                y = tolower(z)\n",
    "                return(y)\n",
    "    }\n",
    "            \n",
    "    x = sapply(x, try.error)\n",
    "    \n",
    "    #Keep lepen > 3 letters\n",
    "    x = gsub(\"le pen\", \"lepen\", x)\n",
    "    x = gsub(\"#primaire\\\\w+ *\", \"\", x)\n",
    "            \n",
    "    #remove all links starting by http\n",
    "    x = gsub('http\\\\S+\\\\s*', '', x)\n",
    "    \n",
    "    # modifying ne and n' to keep negation in cleaned tweet.\n",
    "    x = gsub('ne', 'nenene', x)\n",
    "    x = gsub(\"n'\", 'nenene ', x)\n",
    "    \n",
    "    # replace apostrophes\n",
    "    x = gsub(\"'\", \" \", x)\n",
    "\n",
    "    # remove punctuation except @, #, _, -\n",
    "    x = gsub(\"@\", \"AAAAAAAAAAA\", x)\n",
    "    x = gsub(\"#\", \"BBBBBBBBBBB\", x)\n",
    "    x = gsub(\"_\", \"CCCCCCCCCCC\", x)\n",
    "    x = gsub(\"-\", \"DDDDDDDDDDD\", x)\n",
    "    x = gsub(\"[[:punct:]]\", \" \", x)\n",
    "    x = gsub(\"AAAAAAAAAAA\", \"@\", x)\n",
    "    x = gsub(\"BBBBBBBBBBB\", \"#\", x)\n",
    "    x = gsub(\"CCCCCCCCCCC\", \"_\", x)\n",
    "    x = gsub(\"DDDDDDDDDDD\", \"-\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved @\n",
    "    x = gsub(\"@ \", \"@\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved _\n",
    "    x = gsub(\"_ \", \"_\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved -\n",
    "    x = gsub(\"- \", \"-\", x)\n",
    "    \n",
    "    # remove numbers/Digits\n",
    "    x = gsub(\"[[:digit:]]\", \"\", x)\n",
    "    \n",
    "    # remove tabs\n",
    "    x = gsub(\"[ |\\t]{2,}\", \" \", x)\n",
    "            \n",
    "    # remove blank spaces at the beginning/end\n",
    "    x = gsub(\"^ \", \"\", x)  \n",
    "    x = gsub(\" $\", \"\", x)\n",
    "    \n",
    "    \n",
    "    # As we have already a column indicating if the tweet is a retweet or not \n",
    "    # we can remove \"RT @xxx\" in the tweet header\n",
    "    x = gsub(\"rt @\\\\w+ *\", \"\", x)\n",
    "    \n",
    "    # Remove words of 3 letters or less excepted negation, bon and loi:\n",
    "    x = gsub ('pas', 'paspaspas', x)\n",
    "    x = gsub ('bon', 'bonbonbon', x)\n",
    "    x = gsub ('loi', 'loiloiloi', x)\n",
    "    x = gsub('\\\\b\\\\w{1,3}\\\\s','', x)\n",
    "    x = gsub ('loiloiloi', 'loi', x)\n",
    "    x = gsub ('bonbonbon','bon', x)\n",
    "    x = gsub('nenene', 'ne', x)\n",
    "    x = gsub ('paspaspas', 'pas', x)\n",
    "\n",
    "    x = gsub('bachar', '', x)\n",
    "    x = gsub('assad', 'alassad', x)\n",
    "            \n",
    "    # remove double spaces\n",
    "    x = gsub(\"  \", \" \", x)\n",
    "    x = gsub(\"  \", \" \", x)\n",
    "    return(x)\n",
    "}\n",
    "tweets$originalTweet <- tweets$text                             \n",
    "tweets$text_cleaned_wordcount <- clean_text(tweets$text) #Part II\n",
    "tweets$text_cleaned_sentiment <- clean_text(tweets$text) #Part III\n",
    "tweets$text <- NULL\n",
    "colnames(tweets)[1] <- \"ID\"\n",
    "                             \n",
    "tweets <- tweets[, c(1, 11, 12, 13, 2, 3, 4, 5, 6, 7, 8, 9, 10)]\n",
    "head(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Let's see which are the most used @xxx and replace them with words. Afterward we will delete all the @xxx that will not be replaced.<br> \n",
    "To do that, we created a function called number_Top able to recover words most used according to:\n",
    "<ol>\n",
    "<li>A specific pattern / first argument</li>\n",
    "<li>The N number of words you want to return / second argument</li>\n",
    "<li>The way you want to diplay it: decreasing = TRUE or FALSE / third argument</li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "number_Top = function(column ,at.pattern, number, Topdecreasing){\n",
    "\n",
    "    have.at = grep(x = column, pattern = at.pattern)\n",
    "    at.matches = gregexpr(pattern = at.pattern, text = column[have.at])\n",
    "    extracted.at = regmatches(x = column[have.at], m = at.matches)\n",
    "\n",
    "    # most frequent words\n",
    "    most_f_words = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "    most_f_words = gsub(\" \", \"\", most_f_words)\n",
    "    words = sort(table(unlist(most_f_words)), decreasing=TRUE)\n",
    "    \n",
    "    topWord = head(words, n = number)\n",
    "    topWord = sort(topWord, decreasing=Topdecreasing) \n",
    "    return(topWord)\n",
    "}\n",
    "\n",
    "top40 = number_Top(tweets$text_cleaned_wordcount, \"@\\\\w+ *\", 40, TRUE)\n",
    "top40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_name_text = function(x)\n",
    "{\n",
    "  x = gsub('\\\\S+hamo\\\\S+', 'hamon', x)\n",
    "  x = gsub('\\\\S+ontebou\\\\S+', 'montebourg', x)\n",
    "  x = gsub('\\\\S+vall\\\\S+', 'valls', x)\n",
    "  x = gsub('\\\\S+peillon\\\\S+', 'peillon', x)\n",
    "  x = gsub('\\\\S+rugy\\\\S+', 'derugy', x)\n",
    "  x = gsub('\\\\S+macro\\\\S+', 'macron', x)\n",
    "  x = gsub('\\\\S+francet\\\\S+', 'francetv', x)\n",
    "  x = gsub('\\\\S+pinel\\\\S+', 'pinel', x)\n",
    "  x = gsub('\\\\S+nnahmia\\\\S+', 'bennahmias', x)\n",
    "  x = gsub('\\\\S+eunesavecam\\\\S+', 'montebourg', x) #jeunes avec Arnaud Montebourg\n",
    "  x = gsub('\\\\S+galut\\\\S+', 'galut', x) \n",
    "  x = gsub('\\\\S+donald\\\\S+', 'trump', x)\n",
    "  x = gsub('\\\\S+trump\\\\S+', 'trump', x)\n",
    "  x = gsub('\\\\S+najat\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+vallaud\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+elkacem\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+europe\\\\S+', 'europe', x)\n",
    "  x = gsub('\\\\S+olland\\\\S+', 'hollande', x)\n",
    "  x = gsub('\\\\S+ujada\\\\S+', 'pujadas', x)\n",
    "  x = gsub('\\\\S+ujada\\\\S+', 'pujadas', x)\n",
    "  x = gsub('\\\\S+taubir\\\\S+', 'taubira', x)\n",
    "  x = gsub('\\\\S+sapin\\\\S+', 'sapin', x)\n",
    "  x = gsub('\\\\S+guillaumetc\\\\S+', 'taubira', x)\n",
    "  x = gsub('\\\\S+aubry\\\\S+', 'aubry', x)\n",
    "  x = gsub('\\\\S+compile\\\\S+', 'compile', x)\n",
    "  x = gsub('\\\\S+melenchon\\\\S+', 'melenchon', x)\n",
    "  x = gsub('\\\\S+francei\\\\S+', 'franceinfo', x)\n",
    "  x = gsub('\\\\S+bfm\\\\S+', 'bfmtv', x)\n",
    "  x = gsub('\\\\S+namia\\\\S+', 'namias', x)\n",
    "  x = gsub('\\\\S+vp_\\\\S+', 'peillon', x)\n",
    "  x = gsub('\\\\S+fillon\\\\S+', 'fillon', x)\n",
    "  x = gsub('\\\\S+avecmv\\\\S+', 'valls', x) #Avec Manuel Valls\n",
    "  x = gsub('\\\\S+mlp\\\\S+', 'lepen', x)\n",
    "\n",
    "  x = gsub(\"#\\\\w+ *\", \"\", x)\n",
    "  x = gsub(\"@\\\\w+ *\", \"\", x)\n",
    "}\n",
    "\n",
    "tweets$text_cleaned_wordcount <- clean_name_text(tweets$text_cleaned_wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=4.5 color=\"#206B50\"><center><B>PART II - 1: LIST OF 15 MOST COMMON WORD</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see which are the most used words.<br>\n",
    "First, let's look at the 30 most frequent words. <br>\n",
    "Thanks to the function we created just above we can easily return the 30 most frequent words by juste changing our pattern argument.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top30 = number_Top(tweets$text_cleaned_wordcount, \"[a-zA-Z]\\\\w+ *\", 30, FALSE)\n",
    "barplot(top30, border=NA, las=2, main=\"Top 30 most frequent word\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>We can see in the display of the 30 most frequent word that some of them are not revelant like the preposition 'dans' so we want to clean our text again.<br>\n",
    "We can also do like we did in english analysis creating stopwords_regex.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"tm\")\n",
    "library(tm)\n",
    "stopwords('french')\n",
    "stopwords_regex = paste(stopwords('french'), collapse = '\\\\b|\\\\b')\n",
    "stopwords_regex = paste0('\\\\b', stopwords_regex, '\\\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>\n",
    "Before to clean our text with stopwords regex, we will save our column as 'text_cleaned_sentiment' column name because we will probably need words being in stopwords like 'ne' or 'pas' for sentiment analysis. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets$text_cleaned_sentiment = tweets$text_cleaned_wordcount\n",
    "tweets$text_cleaned_wordcount = stringr::str_replace_all(tweets$text_cleaned_wordcount, stopwords_regex, '')\n",
    "\n",
    "top15 = number_Top(tweets$text_cleaned_wordcount, \"[a-zA-Z]\\\\w+ *\", 15, TRUE)\n",
    "barplot(sort(top15), border=NA, las=2, main=\"Top 15 most frequent without stopwords\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>\n",
    "As there is still not meaningful word we removed it by hand with a clean prepositions function.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_prepo_text = function(x){\n",
    "    x = gsub(\"quand \", \"\", x)\n",
    "    x = gsub(\"faut \", \"\", x)\n",
    "    x = gsub(\"veux \", \"\", x)\n",
    "    x = gsub(\"tre \", \"\", x)\n",
    "    x = gsub(\"tout \", \"\", x)\n",
    "    x = gsub(\"plus \", \"\", x)\n",
    "    x = gsub(\"doit \", \"\", x)\n",
    "    x = gsub(\"fait \", \"\", x)\n",
    "    x = gsub(\"faire \", \"\", x)\n",
    "    x = gsub(\"comme \", \"\", x)\n",
    "    x = gsub(\"tout \", \"\", x)\n",
    "    x = gsub(\"quand \", \"\", x)\n",
    "    x = gsub(\"suis \", \"\", x)\n",
    "}\n",
    "\n",
    "tweets$text_cleaned_wordcount <- clean_prepo_text(tweets$text_cleaned_wordcount)\n",
    "\n",
    "top15 = number_Top(tweets$text_cleaned_wordcount, \"[a-zA-Z]\\\\w+ *\", 15, FALSE)\n",
    "barplot(top15, border=NA, las=2, main=\"Top 15 most frequent & meaningful words\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br>\n",
    "<p><font size=4.5 color=\"#206B50\"><center><B>PART II - 2: LIST OF THE 2 PAIRS OF WORDS HAVING THE HIGHEST CO_OCCURRENCE FREQUENCY</B></center></font></p>\n",
    "\n",
    "<font color=\"#2E1698\" size = 3.2>As we did before we will select, in a first time, more words than necessary in order to have a larger palette of work. <br>\n",
    "So let's recover the top 20 of the most used words, and thanks to the package \"tm\" (need R 3.3.2) find the most correlated words to both of those top 20 having at least a cooccurrence score of 0.3. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "library(tm) # need R 3.3.2\n",
    "docs <- Corpus(VectorSource(tweets$text_cleaned_wordcount))\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "\n",
    "top20 = number_Top(tweets$text_cleaned_wordcount, \"[a-zA-Z]\\\\w+ *\", 20, FALSE)\n",
    "associations = findAssocs(dtm, names(top20), corlimit = 0.3)\n",
    "associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have discover all the associations, we want to generate an algorithm able to organize intelligently those data as a dataframe no matter how is the input / text / subject / ... <br>\n",
    "Once the dataframe is created using '|' as separator we can create easily several columns.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vect = c()\n",
    "\n",
    "for(i in 1:15){\n",
    "    size1 = length(associations[i][[1]])\n",
    "    if(size1 > 0){\n",
    "        size2 = length(associations[i][[1]])\n",
    "        for(j in 1:size2){\n",
    "            vect = c(vect, paste0(names(top20)[i],\"|\" , names(associations[i][[1]][j]), \"|\",associations[i][[1]][[j]][1]))\n",
    "        }  \n",
    "    }\n",
    "}\n",
    "\n",
    "cooccurrence_df = data.frame(vect)\n",
    "head(cooccurrence_df)\n",
    "\n",
    "cooccurrence_df <- data.frame(do.call('rbind', strsplit(as.character(cooccurrence_df$vect),'|',fixed=TRUE)))\n",
    "colnames(cooccurrence_df) <- c(\"word_1\", \"word_2\", \"score\")\n",
    "head(cooccurrence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have a proper dataframe, we can sort it by score and recover the best cooccurrence.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cooccurrence_df = cooccurrence_df[with(cooccurrence_df, order(-as.numeric(score), word_1)), ]\n",
    "head(cooccurrence_df, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=4.5 color=\"#206B50\"><center><B>PART III - 1: GRAPHICAL REPRESENTATION</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>The first graphical representation we'll do is a wordcloud representing what we found in the part above (most used words). To do that we will use wordcloud library which is really representative for our work. Let's see what does it look like in our case.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "library(SnowballC)\n",
    "library(wordcloud)\n",
    "\n",
    "top50 = number_Top(tweets$text_cleaned_wordcount, \"[a-zA-Z]\\\\w+ *\", 50, FALSE)\n",
    "wordcloud(names(top50), top50, min.freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "library(wordcloud2)\n",
    "\n",
    "top100 = number_Top(tweets$text_cleaned_wordcount, \"[a-zA-Z]\\\\w+ *\", 100, FALSE)\n",
    "cat(\"The letterCloud function does not work on Jupyter notebook, if you are using markdown, you can uncomment this line.\")\n",
    "#letterCloud(top100, word = \"PS\", wordSize = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>The first step is to categorize each tweet then we will see the most common word in each category (positive, neutral, negative). So far we have found and traduce a list of positive and negative words, stored into a .txt file.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation tweets function\n",
    "sentence <- tweets$text_cleaned_sentiment\n",
    "\n",
    "score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')\n",
    "{\n",
    "  require(plyr)\n",
    "  require(stringr)\n",
    "  scores <- laply(sentences, function(sentence, pos.words, neg.words){\n",
    "    word.list <- str_split(sentence, ' ')\n",
    "    words <- unlist(word.list)\n",
    "    pos.matches <- match(words, pos.words)\n",
    "    neg.matches <- match(words, neg.words)\n",
    "    pos.matches <- !is.na(pos.matches)\n",
    "    neg.matches <- !is.na(neg.matches)\n",
    "    score <- sum(pos.matches) - sum(neg.matches)\n",
    "    return(score)\n",
    "  }, pos.words, neg.words, .progress=.progress)\n",
    " scores.df <- data.frame(score=scores, text_cleaned_sentiment=sentences)\n",
    " return(scores.df)\n",
    "}\n",
    "\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/Positive.txt\")\n",
    "pos <- scan(yourPath, what='character', comment.char=';') #folder with positive dictionary\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/Negative.txt\")\n",
    "neg <- scan(yourPath, what='character', comment.char=';') #folder with negative dictionary\n",
    "scores <- score.sentiment(sentence, pos, neg, .progress='text')\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total evaluation: positive / negative / neutral\n",
    "stat <- scores\n",
    "stat$created <- tweets$created\n",
    "stat$created <- as.Date(stat$created)\n",
    "stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>As you can see here there is a problem, some tweets are not scored as they should be. For example the word \"incarner\" could be in the 'Positive' dictionary but written as \"incarne\" or \"incarné\". We should use stemming both on our tweets and on our positive/negative dictionaries.</font>\n",
    "<br/>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see what we get using stemming this time!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function will use stemming on a list\n",
    "stemming = function(unstem_list){\n",
    "  # To use stemming we have to convert our list of words into a corpus.\n",
    "  list_corpus <- VCorpus(VectorSource(unstem_list))\n",
    "  list_stem <- tm_map(list_corpus, PlainTextDocument)  # needs to come before stemming\n",
    "  list_stem <- tm_map(list_stem, stemDocument, \"french\")\n",
    "  #Converting the corpus into a dataframe\n",
    "  dataframe_stemmed<-data.frame(text=unlist(sapply(list_stem, `[`, \"content\")), stringsAsFactors=F)\n",
    "  return(dataframe_stemmed)\n",
    "}\n",
    "\n",
    "# This function will use stemming on a dictionary\n",
    "stemming_dictionary = function(path){\n",
    "  yourPath = paste0(getwd(),path)\n",
    "  # First we read the dictionary and store it into a variable\n",
    "  dictionary = read.table(yourPath, sep='\\n')\n",
    "  # There might be some values present more than once, so let's make it unique\n",
    "  dictionary = unique(dictionary)\n",
    "  dictionary = dictionary$V1\n",
    "  dataframe_stemmed = stemming(dictionary)\n",
    "  dataframe_stemmed<-unique(dataframe_stemmed)\n",
    "  return(dataframe_stemmed)\n",
    "}\n",
    "neg_stem = stemming_dictionary(\"/data/NegPos/Negative.txt\")$text                                 \n",
    "head(neg_stem)\n",
    "pos_stem = stemming_dictionary(\"/data/NegPos/Positive.txt\")$text                                 \n",
    "head(pos_stem)\n",
    "                                                                \n",
    "# And now we create 2 txt files containing the words negative and positive after stemming\n",
    "# \n",
    "\n",
    "#write.table(neg_stem, file = \"NegPos/NegativeStem.txt\", append = FALSE, quote = FALSE, sep = \"\\n\",\n",
    "#            na = \"NA\", dec = \".\", row.names = FALSE,\n",
    "#            col.names = FALSE)\n",
    "#write.table(pos_stem, file = \"NegPos/PositiveStem.txt\", append = FALSE, quote = FALSE, sep = \"\\n\",\n",
    "#            na = \"NA\", dec = \".\", row.names = FALSE,\n",
    "#            col.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stemming of the tweets\n",
    "dataframe_stem <- stemming(tweets$text_cleaned_sentiment)\n",
    "\n",
    "#evaluation tweets after stemming function\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/PositiveStem.txt\")\n",
    "pos <- scan(yourPath, what='character', comment.char=';') #folder with positive dictionary\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/NegativeStem.txt\")\n",
    "neg <- scan(yourPath, what='character', comment.char=';') #folder with negative dictionary\n",
    "\n",
    "scores <- score.sentiment(dataframe_stem$text, pos_stem, neg_stem, .progress='text')\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total evaluation: positive / negative / neutral\n",
    " stat <- scores\n",
    " stat$originalTweet <- tweets$originalTweet\n",
    " stat$text_cleaned_sentiment <- scores$text_cleaned_sentiment\n",
    " #stat$text_cleaned_sentiment <- tweets$text_cleaned_sentiment\n",
    " stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n",
    "\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have a score on each tweet, let's study the words' occurence.</font><br>\n",
    "<font color=\"#2E1698\" size = 3.2>We are going to simply see, like we did before, which word is appearing the most in our positive/negative only dataframe taht we are going to create.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Building a dataframe with only positive tweets.\n",
    "positive_tweets = subset(stat, tweet == 'positive')\n",
    "\n",
    "# Let's see which are the word that appear the most:\n",
    "tweet = positive_tweets$text_cleaned_sentiment\n",
    "tweet = gsub(\"une\", \"\", tweet)\n",
    "#Using the function number_top already defined before:\n",
    "top3pos = number_Top(tweet, \"[a-zA-Z]\\\\w+ *\", 3, FALSE)\n",
    "\n",
    "# Same thing with negative tweets\n",
    "negative_tweets = subset(stat, tweet == 'negative')\n",
    "tweet = negative_tweets$text_cleaned_sentiment\n",
    "top3neg = number_Top(tweet, \"[a-zA-Z]\\\\w+ *\", 3, FALSE)\n",
    "\n",
    "#Now we create a dataframe with correct format to be able to plot it with ggplot\n",
    "dfpos = data.frame(top3pos)\n",
    "dfneg = data.frame(top3neg)\n",
    "dfpos$position = c(\"3\", \"2\", \"1\")\n",
    "dfneg$position = c(\"3\", \"2\", \"1\")\n",
    "dfpos$polarity = \"Positive tweets\"\n",
    "dfneg$polarity = 'Negative tweets'\n",
    "df_temp = rbind(dfneg, dfpos)\n",
    "df_temp = df_temp[with(df_temp, order(position, polarity)), ]\n",
    "head(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "df_plot = data.frame(ranking=df_temp$position, \n",
    "                polarity=df_temp$polarity, \n",
    "                freq=sort(df_temp$Freq, decreasing = TRUE),\n",
    "                word = df_temp$Var1\n",
    "               )\n",
    "\n",
    "ggplot(df_plot, aes(ranking, freq, fill = polarity)) \n",
    "+ geom_bar(stat=\"identity\", position = \"dodge\") \n",
    "+ geom_text(aes(label=word), vjust=-0.250, position=position_dodge(width=0.9)) \n",
    "+ labs(title=\"Plot of most frequent words in positive/negative tweets\", x=\"Ranking\", y = \"Frequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Well, we can see after this little study that there is not much differences between 'positive' and 'negative' tweets. The most occuring words are neither positive or negative and are the same as in the top 15 we did before.</font>\n",
    "<br/><br/>\n",
    "<font color=\"#2E1698\" size = 3.2>Now let's see what we get using another strategy:\n",
    "    <ol>\n",
    "      <li>Building a column containing only positive/negative words of a tweet</li>\n",
    "      <li>Analyse this column only and see which words are appearing the most</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "extract_words = function(path, df_tweets){\n",
    "  #Building positive.words column:\n",
    "  yourPath = paste0(getwd(),path)\n",
    "  dictionary <- scan(yourPath, what='character', comment.char=';')\n",
    "  word.list <- str_split(df_tweets$text, \" \")\n",
    "  df_tweets$dictionary.words <- word.list\n",
    "\n",
    "  # this function will create a list of words by comparing if the words in tweet are in the dictionary\n",
    "  dictionary_words = function(sentence){\n",
    "    b = c()\n",
    "    i = 0\n",
    "    for(w in sentence){\n",
    "      if(w %in% dictionary){\n",
    "        i = i + 1\n",
    "        b[i] <- w\n",
    "      }\n",
    "    }\n",
    "    return(b)\n",
    "  }\n",
    "  df_tweets$dictionary.words <- lapply(df_tweets$dictionary.words, dictionary_words)\n",
    "  return(df_tweets)\n",
    "}\n",
    "positive_tweets = extract_words(\"/data/NegPos/PositiveStem.txt\", positive_tweets)\n",
    "negative_tweets = extract_words(\"/data/NegPos/NegativeStem.txt\", negative_tweets)\n",
    "head(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# New cell\n",
    "extract_words = function(path1, path2, df_tweets){\n",
    "  #Building positive.words column:\n",
    "  yourPath = paste0(getwd(),path1)\n",
    "  dictionary1 <- scan(yourPath, what='character', comment.char=';')\n",
    "  yourPath = paste0(getwd(),path2)\n",
    "  dictionary2 <- scan(yourPath, what='character', comment.char=';')\n",
    "  word.list <- str_split(df_tweets$text, \" \")\n",
    "  df_tweets$dictionary.words <- word.list\n",
    "\n",
    "  # this function will create a list of words by comparing if the words in tweet are in the dictionary\n",
    "  dictionary_words = function(sentence){\n",
    "    b = c()\n",
    "    i = 0\n",
    "    for(w in sentence){\n",
    "      if(w %in% dictionary1 || w %in% dictionary2){\n",
    "        i = i + 1\n",
    "        b[i] <- w\n",
    "      }\n",
    "    }\n",
    "    return(b)\n",
    "  }\n",
    "  df_tweets$dictionary.words <- lapply(df_tweets$dictionary.words, dictionary_words)\n",
    "  return(df_tweets)\n",
    "}\n",
    "stat = extract_words(\"/data/NegPos/PositiveStem.txt\",\"/data/NegPos/NegativeStem.txt\", stat)\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# New cell\n",
    "words.vector<-unlist(stat$dictionary.words)\n",
    "freq.list<-table(words.vector)\n",
    "sorted.freq.list<-sort(freq.list, decreasing=TRUE)\n",
    "mfw = head(sorted.freq.list, n = 100, decreasing = FALSE)\n",
    "df_words_freq = data.frame(mfw)\n",
    "a = 0\n",
    "path_pos = \"/data/NegPos/PositiveStem.txt\"\n",
    "path_neg = \"/data/NegPos/NegativeStem.txt\"\n",
    "yourPath = paste0(getwd(), path_pos)\n",
    "pos_dictionary <- scan(yourPath, what='character', comment.char=';')\n",
    "yourPath = paste0(getwd(), path_neg)\n",
    "neg_dictionary <- scan(yourPath, what='character', comment.char=';')\n",
    "for(i in df_words_freq$words.vector){\n",
    "    a = a + 1\n",
    "    if(i %in% pos_dictionary){\n",
    "        df_words_freq$polarity[a] = \"pos\"\n",
    "    }\n",
    "    else if(i %in% neg_dictionary){\n",
    "        df_words_freq$polarity[a] = \"neg\"\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "    # Liste à inverser...:    \n",
    "barplot(sort(head(mfw, 20)), border=NA, las=2, main=\"Top 20 most frequent word with polarity\", \n",
    "        cex.main=1, horiz=TRUE, col= ifelse(df_words_freq$polarity == \"pos\", 'green', 'red'), cex.names=0.65)\n",
    "legend(\"bottomright\", c(\"positive\",\"negative\"), pch=c(19,19), col=c(\"green\",\"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#2E1698\" size = 3.2>\n",
    "We can now create a wordcloud displaying positives and negatives most used words separatly.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_pos_words = df_words_freq[which(df_words_freq$polarity == \"pos\"),]\n",
    "df_neg_words = df_words_freq[which(df_words_freq$polarity == \"neg\"),]\n",
    "df_pos_words = head(df_pos_words, 20)\n",
    "df_neg_words = head(df_neg_words, 20)\n",
    "df_pos_words$Freq <- NULL\n",
    "df_pos_words$polarity <- NULL\n",
    "df_neg_words$Freq <- NULL\n",
    "df_neg_words$polarity <- NULL\n",
    "all = c(df_pos_words, df_neg_words)\n",
    "\n",
    "# create corpus\n",
    "corpus = Corpus(VectorSource(all))\n",
    "\n",
    "# create term-document matrix\n",
    "tdm = TermDocumentMatrix(corpus)\n",
    "\n",
    "# convert as matrix\n",
    "tdm = as.matrix(tdm)\n",
    "\n",
    "# add column names\n",
    "colnames(tdm) = c(\"Positive\", \"Negative\")\n",
    "\n",
    "comparison.cloud(tdm, random.order=FALSE, colors = c(\"darkred\", \"darkblue\", \"blue\", \"darkblue\"), \n",
    "                 title.size=1.5, max.words=50, scale=c(2,.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=4.5 color=\"#206B50\"><center><B>PART III - 2: Indicate 3 most frequent words in each category</B></center></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Plotting our result\n",
    "plot_words = function(df_tweets){\n",
    "  words.vector<-unlist(df_tweets$dictionary.words)\n",
    "  freq.list<-table(words.vector)\n",
    "  sorted.freq.list<-sort(freq.list, decreasing=TRUE)\n",
    "  mfw = head(sorted.freq.list, n = 5)\n",
    "  return(mfw)\n",
    "}\n",
    "mfw_pos = plot_words(positive_tweets)\n",
    "mfw_neg = plot_words(negative_tweets)\n",
    "\n",
    "#Now we create a dataframe with correct format to be able to plot it with ggplot\n",
    "dfpos = data.frame(mfw_pos)\n",
    "dfneg = data.frame(mfw_neg)\n",
    "dfpos$position = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n",
    "dfneg$position = c(\"1\", \"2\", \"3\", \"4\", \"5\")\n",
    "dfpos$polarity = \"Positive words\"\n",
    "dfneg$polarity = 'Negative words'\n",
    "df_temp = rbind(dfneg, dfpos)\n",
    "df_temp = df_temp[with(df_temp, order(position, polarity)), ]\n",
    "df_temp\n",
    "\n",
    "\n",
    "df_plot = data.frame(ranking=df_temp$position, \n",
    "                polarity=df_temp$polarity, \n",
    "                freq=df_temp$Freq,\n",
    "                word = df_temp$words.vector\n",
    "               )\n",
    "\n",
    "ggplot(df_plot, aes(ranking, freq, fill = polarity)) + geom_bar(stat=\"identity\", position = \"dodge\") + \n",
    "  geom_text(aes(label=word), vjust=-0.250, position=position_dodge(width=0.9)) + \n",
    "labs(title=\"Plot of most frequent words in positive/negative tweets\", x=\"Ranking\", y = \"Frequence\")\n",
    "# Simple Bar Plot \n",
    "#barplot(mfw_pos, main=\"Top positive (stemmed) words\", xlab=\"positive Words\", col = \"darkblue\")\n",
    "#barplot(mfw_neg, main=\"Top negative (stemmed) words\", xlab=\"negative Words\", col = \"darkblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>So, we can see here for the positive stemmed words with the highest appearence frequency in positive tweets. We have:\n",
    "    <ol>\n",
    "      <li>'fait', the word with the highest frequency.</li>\n",
    "      <li>'plus'</li>\n",
    "      <li>'droit', with stemming it regroups also words of the same root, like: \n",
    "        <ol>\n",
    "          <li>droits</li>\n",
    "          <li>droite</li>\n",
    "          <li>droites</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'bien'</li>\n",
    "      <li>'sérieux'</li>\n",
    "    </ol>\n",
    "</font>\n",
    "<br/>\n",
    "<font color=\"#2E1698\" size = 3.2>Same thing for negative stemmed words with the highest appearence frequency in negative tweets. We have:\n",
    "    <ol>\n",
    "      <li>'femm', the word with the highest frequency which is not even a real word but regroup words with roots like:\n",
    "        <ol>\n",
    "          <li>femmes</li>\n",
    "          <li>femme</li>\n",
    "        </ol></li>\n",
    "      <li>'flou', that can regroup:\n",
    "        <ol>\n",
    "          <li>flou</li>\n",
    "          <li>flous</li>\n",
    "          <li>And maybe mispelled words like 'floux'</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'lutt' that may regroup:\n",
    "        <ol>\n",
    "            <li>lutte</li>\n",
    "            <li>luttes</li>\n",
    "            <li>lutter, etc.</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'question'</li>\n",
    "      <li>'seul'</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=4.5 color=\"#206B50\"><center><B>PART III - 3: Compare the results of the two approaches</B></center></font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R 3.3",
   "language": "R",
   "name": "ir33"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
