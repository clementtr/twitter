{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=6 color=\"#2E1698\"><em><u><center>Data Processing</center></u></em></font></p><br>\n",
    "<p><font size=3.2 color=\"#2E1698\"><i><u>Introduction:</u></i> explain our data blablabla we are here aiming to manipulate the data that we generated before.</font><p>\n",
    "<br>\n",
    "<font color=\"#206B50\" size = 3><center>**SUMMARY**</center></font><br>\n",
    "<font size=3.2 color=\"#2E1698\">\n",
    "<ol>\n",
    "      <li>Data cleaning</li>\n",
    "      <li>Provide a list of the 15 most common words</li>\n",
    "      <li>Provide a list of the 2 pairs of words having the highest co-occurrence frequency</li>\n",
    "      <li>Build a graphical representation of the most frequent words with their polarity (pos/neg or anger/joy/fear/...)</li>\n",
    "      <li>Indicate the 3 most frequent representatives words in each category</li>\n",
    "      <li>Compare the results of the two approaches</li>\n",
    "    </ol>\n",
    "</font>\n",
    "<br>\n",
    "<p><font size=3 color=\"#206B50\"><center><B>PART I: DATA CLEANING</B></center></font></p>\n",
    "<font size=3 color=\"#2E1698\">First things first, let's import the csv file. We are using french tweets this is why we need to specify the UTF-8 encoding. To have a better idea of our database we decided to show the first 5 lines.</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yourPath = paste0(getwd(),\"/data/debat_primaire_20000.csv\")\n",
    "tweets = read.csv(yourPath, encoding=\"UTF-8\")\n",
    "print(dim(tweets))\n",
    "head(tweets, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font color=\"#2E1698\" size = 3.2>As you can see, our data frame contains <font color=\"red\">17</font> columns and <font color=\"red\">20 000</font> rows, let's see the 10 firsts rows.</font><br></p>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see if all the columns have multiple values, or if some are useless.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"UNIQUE\", \"\\n\")\n",
    "cat(\"Favorited: \",head(unique(tweets$favorited)), \"\\n\")\n",
    "cat(\"FavoriteCount: \",head(unique(tweets$favoriteCount)), \"\\n\")\n",
    "cat(\"ReplyToSN: \",head(unique(tweets$replyToSN)), \"\\n\")\n",
    "cat(\"ReplyToUID: \",head(unique(tweets$replyToUID)), \"\\n\")\n",
    "cat(\"Id: \",head(unique(tweets$id)), \"\\n\")\n",
    "cat(\"IsRetweet: \",head(unique(tweets$isRetweet)), \"\\n\")\n",
    "cat(\"Lattitude: \",head(unique(tweets$latitude)), \"\\n\")\n",
    "cat(\"Longitude: \",head(unique(tweets$longitude)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Favorited TRUE: \", length(which(tweets$favorited == \"TRUE\")), \"\\n\")\n",
    "cat(\"Favorited FALSE: \", length(which(tweets$favorited == \"FALSE\")), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font  \n",
    "color=\"#2E1698\" size = 3.2>We can see here that there is no TRUE value for favorited, only FALSE. favorited is useless though.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"No favoritedCount: \", length(which(tweets$favoriteCount == 0)), \"\\n\")\n",
    "cat(\"At least one favoritedCount: \", length(which(tweets$favoriteCount != 0)), \"\\n\")\n",
    "cat(\"Percentage of tweets that have favoritedCount\",(3919/20000)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>The favoriteCount have multiple values, 20% of the are not 0 we better keep this column. It is maybe a significative data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Tweets containing longitude information: \", length(which(tweets$longitude != \"NA\")), \"\\n\")\n",
    "cat(\"Tweets containing latitude information: \", length(which(tweets$latitude != \"NA\")), \"\\n\")\n",
    "cat(\"Percentage of tweets containing infos: \", 9/20000*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>There is only 9 tweets over 20 000 that contains latitude and longitude, this represents only 0.045% of the tweets, this info can be considered as useless, and we can delete this two columns too.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Tweets with replyToSN not null\", length(which(tweets$replyToSN != \"NA\")),\"\\n\")\n",
    "cat(\"Tweets with replyToUID not null\", length(which(tweets$replyToUID != 'NA')), \"\\n\")\n",
    "cat (\"Tweets with replyToSID not null\", length(which(tweets$replyToSID != 'NA')), \"\\n\")\n",
    "cat(\"Percentage of tweets containing replyToSN/UID\", 698/20000*100, \"%\", \"\\n\")\n",
    "cat(\"Percentage of tweets containing replyToSID\", 445/20000*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>There is only about 3.5% of the replytoSN and replyToUID data that are not NA, we can delete these two columns as they don't seem to be interesting to study. Same thing for the replyToSID, with less than 2.5%. <br><br>\n",
    "Let's delete these useless columns!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets <- subset(tweets, select=-c(replyToSN,replyToUID, replyToSID, latitude, longitude, favorited))\n",
    "head(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>If we want to use the text, it have to be cleaned first</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_text = function(x)\n",
    "{\n",
    "    #To convert the text in lowercase\n",
    "    try.error = function(z)\n",
    "    {\n",
    "        y = NA\n",
    "        try_error = tryCatch(tolower(z), error=function(e) e)\n",
    "            if (!inherits(try_error, \"error\"))\n",
    "                y = tolower(z)\n",
    "                return(y)\n",
    "    }\n",
    "            \n",
    "    x = sapply(x, try.error)\n",
    "    \n",
    "    #Keep lepen > 3 letters\n",
    "    x = gsub(\"le pen\", \"lepen\", x)\n",
    "    x = gsub(\"#primaire\\\\w+ *\", \"\", x)\n",
    "            \n",
    "     #remove all links starting by http\n",
    "    x = gsub('http\\\\S+\\\\s*', '', x)\n",
    "    x = gsub(\"n'\", 'nenene', x)\n",
    "    # replace apostrophes\n",
    "    x = gsub(\"'\", \" \", x)\n",
    "\n",
    "    # remove punctuation except @, #, _, -\n",
    "    x = gsub(\"@\", \"AAAAAAAAAAA\", x)\n",
    "    x = gsub(\"#\", \"BBBBBBBBBBB\", x)\n",
    "    x = gsub(\"_\", \"CCCCCCCCCCC\", x)\n",
    "    x = gsub(\"-\", \"DDDDDDDDDDD\", x)\n",
    "    x = gsub(\"[[:punct:]]\", \" \", x)\n",
    "    x = gsub(\"AAAAAAAAAAA\", \"@\", x)\n",
    "    x = gsub(\"BBBBBBBBBBB\", \"#\", x)\n",
    "    x = gsub(\"CCCCCCCCCCC\", \"_\", x)\n",
    "    x = gsub(\"DDDDDDDDDDD\", \"-\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved @\n",
    "    x = gsub(\"@ \", \"@\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved _\n",
    "    x = gsub(\"_ \", \"_\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved -\n",
    "    x = gsub(\"- \", \"-\", x)\n",
    "    \n",
    "    # remove numbers/Digits\n",
    "    x = gsub(\"[[:digit:]]\", \"\", x)\n",
    "    \n",
    "    # remove tabs\n",
    "    x = gsub(\"[ |\\t]{2,}\", \" \", x)\n",
    "            \n",
    "    # remove blank spaces at the beginning/end\n",
    "    x = gsub(\"^ \", \"\", x)  \n",
    "    x = gsub(\" $\", \"\", x)\n",
    "    \n",
    "    \n",
    "    # As we have already a column indicating if the tweet is a retweet or not \n",
    "    # we can remove \"RT @xxx\" in the tweet header\n",
    "    x = gsub(\"rt @\\\\w+ *\", \"\", x)\n",
    "    \n",
    "    # Remove words of 3 letters or less excepted negation:\n",
    "    x = gsub('ne', 'nenene', x)\n",
    "    x = gsub ('pas', 'paspaspas', x)\n",
    "    x = gsub ('bon', 'bonbonbon', x)\n",
    "    x = gsub ('loi', 'loiloiloi', x)\n",
    "    x = gsub('\\\\b\\\\w{1,3}\\\\s','', x)\n",
    "    x = gsub ('loiloiloi', 'loi', x)\n",
    "    x = gsub ('bonbonbon','bon', x)\n",
    "    x = gsub('nenene', 'ne', x)\n",
    "    x = gsub ('paspaspas', 'pas', x)\n",
    "\n",
    "    x = gsub('bachar', '', x)\n",
    "    x = gsub('assad', 'alassad', x)\n",
    "            \n",
    "    # remove double spaces\n",
    "    x = gsub(\"  \", \" \", x)\n",
    "    x = gsub(\"  \", \" \", x)\n",
    "    return(x)\n",
    "}\n",
    "tweets$originalTweet <- tweets$text                             \n",
    "tweets$text <- clean_text(tweets$text)\n",
    "tweets <- tweets[, c(1, 12, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)]\n",
    "head(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Let's see which are the most used @xxx and replace them with words. Afterward we will delete all the @xxx that will not be replaced.<br> \n",
    "To do that, we created a function called number_Top able to recover words most used according to:\n",
    "<ol>\n",
    "<li>A specific pattern / first argument</li>\n",
    "<li>The N number of words you want to return / second argument</li>\n",
    "<li>The way you want to diplay it: decreasing = TRUE or FALSE / third argument</li>\n",
    "</ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "number_Top = function(column ,at.pattern, number, Topdecreasing){\n",
    "\n",
    "    have.at = grep(x = column, pattern = at.pattern)\n",
    "    at.matches = gregexpr(pattern = at.pattern, text = column[have.at])\n",
    "    extracted.at = regmatches(x = column[have.at], m = at.matches)\n",
    "\n",
    "    # most frequent words\n",
    "    most_f_words = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "    most_f_words = gsub(\" \", \"\", most_f_words)\n",
    "    words = sort(table(unlist(most_f_words)), decreasing=TRUE)\n",
    "    \n",
    "    topWord = head(words, n = number)\n",
    "    topWord = sort(topWord, decreasing=Topdecreasing) \n",
    "    return(topWord)\n",
    "}\n",
    "\n",
    "top40 = number_Top(tweets$text, \"@\\\\w+ *\", 40, TRUE)\n",
    "top40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_name_text = function(x)\n",
    "{\n",
    "  x = gsub('\\\\S+hamo\\\\S+', 'hamon', x)\n",
    "  x = gsub('\\\\S+ontebou\\\\S+', 'montebourg', x)\n",
    "  x = gsub('\\\\S+vall\\\\S+', 'valls', x)\n",
    "  x = gsub('\\\\S+peillon\\\\S+', 'peillon', x)\n",
    "  x = gsub('\\\\S+rugy\\\\S+', 'derugy', x)\n",
    "  x = gsub('\\\\S+macro\\\\S+', 'macron', x)\n",
    "  x = gsub('\\\\S+francet\\\\S+', 'francetv', x)\n",
    "  x = gsub('\\\\S+pinel\\\\S+', 'pinel', x)\n",
    "  x = gsub('\\\\S+nnahmia\\\\S+', 'bennahmias', x)\n",
    "  x = gsub('\\\\S+eunesavecam\\\\S+', 'montebourg', x) #jeunes avec Arnaud Montebourg\n",
    "  x = gsub('\\\\S+galut\\\\S+', 'galut', x) \n",
    "  x = gsub('\\\\S+donald\\\\S+', 'trump', x)\n",
    "  x = gsub('\\\\S+trump\\\\S+', 'trump', x)\n",
    "  x = gsub('\\\\S+najat\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+vallaud\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+elkacem\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+europe\\\\S+', 'europe', x)\n",
    "  x = gsub('\\\\S+olland\\\\S+', 'hollande', x)\n",
    "  x = gsub('\\\\S+ujada\\\\S+', 'pujadas', x)\n",
    "  x = gsub('\\\\S+ujada\\\\S+', 'pujadas', x)\n",
    "  x = gsub('\\\\S+taubir\\\\S+', 'taubira', x)\n",
    "  x = gsub('\\\\S+sapin\\\\S+', 'sapin', x)\n",
    "  x = gsub('\\\\S+guillaumetc\\\\S+', 'taubira', x)\n",
    "  x = gsub('\\\\S+aubry\\\\S+', 'aubry', x)\n",
    "  x = gsub('\\\\S+compile\\\\S+', 'compile', x)\n",
    "  x = gsub('\\\\S+melenchon\\\\S+', 'melenchon', x)\n",
    "  x = gsub('\\\\S+francei\\\\S+', 'franceinfo', x)\n",
    "  x = gsub('\\\\S+bfm\\\\S+', 'bfmtv', x)\n",
    "  x = gsub('\\\\S+namia\\\\S+', 'namias', x)\n",
    "  x = gsub('\\\\S+vp_\\\\S+', 'peillon', x)\n",
    "  x = gsub('\\\\S+fillon\\\\S+', 'fillon', x)\n",
    "  x = gsub('\\\\S+avecmv\\\\S+', 'valls', x) #Avec Manuel Valls\n",
    "  x = gsub('\\\\S+mlp\\\\S+', 'lepen', x)\n",
    "\n",
    "  x = gsub(\"#\\\\w+ *\", \"\", x)\n",
    "  x = gsub(\"@\\\\w+ *\", \"\", x)\n",
    "}\n",
    "\n",
    "tweets$text <- clean_name_text(tweets$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART II: LIST OF 15 MOST COMMON WORD</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see which are the most used words.<br>\n",
    "First, let's look at the 30 most frequent words. <br>\n",
    "Thanks to the function we created just above we can easily return the 30 most frequent words by juste changing our pattern argument.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top30 = number_Top(tweets$text, \"[a-zA-Z]\\\\w+ *\", 30, FALSE)\n",
    "barplot(top30, border=NA, las=2, main=\"Top 30 most frequent word\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>We can see in the display of the 30 most frequent word that some of them are not revelant like the preposition 'dans' so we want to clean our text again</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_prepo_text = function(x){\n",
    "    x = gsub (\"ne \", \"\", x)\n",
    "    x = gsub (\"pas\", \"\", x)\n",
    "    x = gsub(\"dans \", \"\", x)\n",
    "    x = gsub(\"pour \", \"\", x)\n",
    "    x = gsub(\"quand \", \"\", x)\n",
    "    x = gsub(\"avec \", \"\", x)\n",
    "    x = gsub(\"mais \", \"\", x)\n",
    "    x = gsub(\"tre \", \"\", x)\n",
    "    x = gsub(\"vous \", \"\", x)\n",
    "    x = gsub(\"nous \", \"\", x)\n",
    "    x = gsub(\"comme \", \"\", x)\n",
    "    x = gsub(\"plus \", \"\", x)\n",
    "    x = gsub(\"tout \", \"\", x)\n",
    "    x = gsub(\"sont \", \"\", x)\n",
    "    x = gsub(\"veux \", \"\", x)\n",
    "    x = gsub(\"doit \", \"\", x)\n",
    "    x = gsub(\"faut \", \"\", x)\n",
    "    x = gsub(\"fait \", \"\", x)\n",
    "    x = gsub(\"faire \", \"\", x)\n",
    "    x = gsub(\"suis \", \"\", x)\n",
    "}\n",
    "\n",
    "column_cleaned_prepo <- clean_prepo_text(tweets$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we keep only most frequent meaninfgul words let's see the top 15 always by using our magic function !</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top15 = number_Top(column_cleaned_prepo, \"[a-zA-Z]\\\\w+ *\", 15, FALSE)\n",
    "barplot(top15, border=NA, las=2, main=\"Top 15 most frequent word meaningful\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br>\n",
    "<p><font size=3 color=\"#206B50\"><center><B>PART III: LIST OF THE 2 PAIRS OF WORDS HAVING THE HIGHEST CO_OCCURRENCE FREQUENCY</B></center></font></p>\n",
    "\n",
    "<font color=\"#2E1698\" size = 3.2>As we did before we will select, in a first time, more words than necessary in order to have a larger palette of work. <br>\n",
    "So let's recover the top 20 of the most used words, and thanks to the package \"tm\" (need R 3.3.2) find the most correlated words to both of those top 20 having at least a cooccurrence score of 0.3. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"tm\")\n",
    "library(tm) # need R 3.3.2\n",
    "docs <- Corpus(VectorSource(tweets$text))\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "\n",
    "top20 = number_Top(column_cleaned_prepo, \"[a-zA-Z]\\\\w+ *\", 20, FALSE)\n",
    "associations = findAssocs(dtm, names(top20), corlimit = 0.3)\n",
    "associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have discover all the associations, we want to generate an algorithm able to organize intelligently those data as a dataframe no matter how is the input / text / subject / ... <br>\n",
    "Once the dataframe is created using '|' as separator we can create easily several columns.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vect = c()\n",
    "\n",
    "for(i in 1:15){\n",
    "    size1 = length(associations[i][[1]])\n",
    "    if(size1 > 0){\n",
    "        size2 = length(associations[i][[1]])\n",
    "        for(j in 1:size2){\n",
    "            vect = c(vect, paste0(names(top20)[i],\"|\" , names(associations[i][[1]][j]), \"|\",associations[i][[1]][[j]][1]))\n",
    "        }  \n",
    "    }\n",
    "}\n",
    "\n",
    "cooccurrence_df = data.frame(vect)\n",
    "head(cooccurrence_df)\n",
    "\n",
    "cooccurrence_df <- data.frame(do.call('rbind', strsplit(as.character(cooccurrence_df$vect),'|',fixed=TRUE)))\n",
    "colnames(cooccurrence_df) <- c(\"word_1\", \"word_2\", \"score\")\n",
    "head(cooccurrence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have a proper dataframe, we can sort it by score and recover the best cooccurrence.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cooccurrence_df = cooccurrence_df[with(cooccurrence_df, order(-as.numeric(score), word_1)), ]\n",
    "head(cooccurrence_df, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART III: GRAPHICAL REPRESENTATION</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>The first graphical representation we'll do is a wordcloud representing what we found in the part above (most used words). To do that we will use wordcloud library which is really representative for our work. Let's see what does it look like in our case.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"wordcloud\")\n",
    "#install.packages(\"SnowballC\")\n",
    "library(SnowballC)\n",
    "library(wordcloud)\n",
    "\n",
    "top50 = number_Top(column_cleaned_prepo, \"[a-zA-Z]\\\\w+ *\", 50, FALSE)\n",
    "wordcloud(names(top50), top50, min.freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "require(devtools)\n",
    "install_github(\"lchiffon/wordcloud2\")\n",
    "library(wordcloud2)\n",
    "\n",
    "top100 = number_Top(column_cleaned_prepo, \"[a-zA-Z]\\\\w+ *\", 100, FALSE)\n",
    "cat(\"The letterCloud function does not work on Jupyter notebook, if you are using markdown, you can uncomment this line.\")\n",
    "#letterCloud(top100, word = \"PS\", wordSize = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART V: Indicate 3 most frequent words in each category</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>The first step is to categorize each tweet then we will see the most common word in each category (positive, neutral, negative). So far we have found and traduce a list of positive and negative words, stored into a .txt file.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation tweets function\n",
    "#install.packages(\"plyr\")\n",
    "\n",
    "sentence <- tweets$text\n",
    "\n",
    "score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')\n",
    "{\n",
    "  require(plyr)\n",
    "  require(stringr)\n",
    "  scores <- laply(sentences, function(sentence, pos.words, neg.words){\n",
    "    word.list <- str_split(sentence, ' ')\n",
    "    words <- unlist(word.list)\n",
    "    pos.matches <- match(words, pos.words)\n",
    "    neg.matches <- match(words, neg.words)\n",
    "    pos.matches <- !is.na(pos.matches)\n",
    "    neg.matches <- !is.na(neg.matches)\n",
    "    score <- sum(pos.matches) - sum(neg.matches)\n",
    "    return(score)\n",
    "  }, pos.words, neg.words, .progress=.progress)\n",
    " scores.df <- data.frame(score=scores, text=sentences)\n",
    " return(scores.df)\n",
    "}\n",
    "\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/Positive.txt\")\n",
    "pos <- scan(yourPath, what='character', comment.char=';') #folder with positive dictionary\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/Negative.txt\")\n",
    "neg <- scan(yourPath, what='character', comment.char=';') #folder with negative dictionary\n",
    "scores <- score.sentiment(sentence, pos, neg, .progress='text')\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total evaluation: positive / negative / neutral\n",
    "stat <- scores\n",
    "stat$created <- tweets$created\n",
    "stat$created <- as.Date(stat$created)\n",
    "stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>As you can see here there is a problem, some tweets are not scored as they should be. For example the word \"incarner\" could be in the 'Positive' dictionary but written as \"incarne\" or \"incarné\". We should use stemming both on our tweets and on our positive/negative dictionaries.</font>\n",
    "<br/>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see what we get using stemming this time!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yourPath = paste0(getwd(),\"/data/NegPos/Negative.txt\")\n",
    "# First we read the negative dictionary and store it into a variable\n",
    "neg = read.table(yourPath, sep='\\n')\n",
    "# There might be some values present more than once, so let's make it unique\n",
    "neg = unique(neg)\n",
    "neg = neg$V1\n",
    "# To use stemming we have to convert our list of words into a corpus.\n",
    "negative = VCorpus(VectorSource(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Same thing here for the positive dictionary\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/Positive.txt\")\n",
    "pos = read.table(yourPath, sep='\\n')\n",
    "pos = unique(pos)\n",
    "pos = pos$V1\n",
    "positive = VCorpus(VectorSource(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Stemming of the positive dictionary\n",
    "pos_stem <- tm_map(positive, PlainTextDocument)  # needs to come before stemming\n",
    "pos_stem <- tm_map(pos_stem, stemDocument, \"french\")\n",
    "#Converting the corpus into a dataframe\n",
    "dataframe_pos<-data.frame(text=unlist(sapply(pos_stem, `[`, \"content\")), stringsAsFactors=F)\n",
    "dataframe_pos<-unique(dataframe_pos)\n",
    "\n",
    "#Stemming of the negative dictionary                                                       \n",
    "neg_stem <- tm_map(negative, PlainTextDocument)  # needs to come before stemming\n",
    "neg_stem <- tm_map(neg_stem, stemDocument, \"french\")\n",
    "#Converting the corpus into a dataframe\n",
    "dataframe_neg<-data.frame(text=unlist(sapply(neg_stem, `[`, \"content\")), stringsAsFactors=F)\n",
    "dataframe_neg<-unique(dataframe_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's put the dataframes into a proper format\n",
    "z_pos = c(dataframe_pos$text)\n",
    "z_neg = c(dataframe_neg$text)\n",
    "head(z_pos)\n",
    "head(z_neg)\n",
    "\n",
    "# And now we create 2 txt files containing the words negative and positive after stemming\n",
    "\n",
    "#write.table(z_neg, file = \"NegPos/NegativeStem.txt\", append = FALSE, quote = FALSE, sep = \"\\n\",\n",
    "#            na = \"NA\", dec = \".\", row.names = FALSE,\n",
    "#            col.names = FALSE)\n",
    "#write.table(z_pos, file = \"NegPos/PositiveStem.txt\", append = FALSE, quote = FALSE, sep = \"\\n\",\n",
    "#            na = \"NA\", dec = \".\", row.names = FALSE,\n",
    "#            col.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stemming of the tweets\n",
    "txt <- tweets$text\n",
    "txt = VCorpus(VectorSource(txt))\n",
    "s <- tm_map(txt, PlainTextDocument)  # needs to come before stemming\n",
    "s <- tm_map(s, stemDocument, \"french\")\n",
    "                                                  \n",
    "dataframe_stem<-data.frame(text=unlist(sapply(s, `[`, \"content\")), stringsAsFactors=F)\n",
    "head(dataframe_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation tweets after stemming function\n",
    "\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/PositiveStem.txt\")\n",
    "pos <- scan(yourPath, what='character', comment.char=';') #folder with positive dictionary\n",
    "\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/NegativeStem.txt\")\n",
    "neg <- scan(yourPath, what='character', comment.char=';') #folder with negative dictionary\n",
    "head(neg)\n",
    "\n",
    "scores <- score.sentiment(dataframe_stem$text, z_pos, z_neg, .progress='text')\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total evaluation: positive / negative / neutral\n",
    " stat <- scores\n",
    " stat$originalTweet <- tweets$originalTweet\n",
    " stat$cleanTweet <- tweets$text\n",
    " stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n",
    "\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have a score on each tweet, let's study the words' occurence.</font><br>\n",
    "<font color=\"#2E1698\" size = 3.2>We are going to simply see, like we did before, which word is appearing the most in our positive/negative only dataframe taht we are going to create.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building a dataframe with only positive tweets.\n",
    "positive_tweets = subset(stat, tweet == 'positive')\n",
    "head(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's see which are the word that appear the most:\n",
    "tweet = positive_tweets$cleanTweet\n",
    "tweet = gsub(\"une\", \"\", tweet)\n",
    "#Using the function number_top already defined before:\n",
    "top3 = number_Top(tweet, \"[a-zA-Z]\\\\w+ *\", 3, FALSE)\n",
    "barplot(top3, border=NA, las=2, main=\"Top 3 most frequent word in positive tweets\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Building a dataframe with only negative tweets.\n",
    "negative_tweets = subset(stat, tweet == 'negative')\n",
    "head(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's see which are the word that appear the most (again):\n",
    "tweet = negative_tweets$cleanTweet\n",
    "\n",
    "#Using the function number_top already defined before:\n",
    "top3 = number_Top(tweet, \"[a-zA-Z]\\\\w+ *\", 3, FALSE)\n",
    "barplot(top3, border=NA, las=2, main=\"Top 3 most frequent words in negative tweets\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Well, we can see after this little study that there is not much differences between 'positive' and 'negative' tweets. The most occuring words are neither positive or negative and are the same as in the top 15 we did before.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now let's see what we get using another strategy:\n",
    "    <ol>\n",
    "      <li>Building a column containing only positive/negative words of a tweet</li>\n",
    "      <li>Analyse this column only and see which words are appearing the most</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Building positive.words column:\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/PositiveStem.txt\")\n",
    "pos <- scan(yourPath, what='character', comment.char=';')\n",
    "word.list <- str_split(positive_tweets$text, \" \")\n",
    "positive_tweets$positive.words <- word.list\n",
    "\n",
    "# this function will create a list of words by comparing if the words in tweet are in the dictionary\n",
    "positive_words = function(sentence){\n",
    "    b = c()\n",
    "    i = 0\n",
    "    for(w in sentence){\n",
    "        if(w %in% pos){\n",
    "            i = i + 1\n",
    "            b[i] <- w\n",
    "        }\n",
    "    }\n",
    "    return(b)\n",
    "}\n",
    "positive_tweets$positive.words <- lapply(positive_tweets$positive.words, positive_words)\n",
    "head(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Plotting our result\n",
    "words.vector<-unlist(positive_tweets$positive.words)\n",
    "freq.list<-table(words.vector)\n",
    "sorted.freq.list<-sort(freq.list, decreasing=TRUE)\n",
    "mfw = head(sorted.freq.list, n = 5)\n",
    "\n",
    "# Simple Bar Plot \n",
    "barplot(mfw, main=\"Top positive (stemmed) words\", xlab=\"positive Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>So, we can see here stemmed words with the highest appearence frequency in positive tweets. We have:\n",
    "    <ol>\n",
    "      <li>'fait', the word with the highest frequency.</li>\n",
    "      <li>'plus'</li>\n",
    "      <li>'droit', with stemming it regroups also words of the same root, like: \n",
    "        <ol>\n",
    "          <li>droits</li>\n",
    "          <li>droite</li>\n",
    "          <li>droites</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'bien'</li>\n",
    "      <li>'sérieux'</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Building negative.words column:\n",
    "yourPath = paste0(getwd(),\"/data/NegPos/NegativeStem.txt\")\n",
    "neg <- scan(yourPath, what='character', comment.char=';')\n",
    "word.list <- str_split(negative_tweets$text, \" \")\n",
    "negative_tweets$negative.words <- word.list\n",
    "\n",
    "# this function will create a list of words by comparing if the words in tweet are in the dictionary\n",
    "negative_words = function(sentence){\n",
    "    b = c()\n",
    "    i = 0\n",
    "    for(w in sentence){\n",
    "        if(w %in% neg){\n",
    "            i = i + 1\n",
    "            b[i] <- w\n",
    "        }\n",
    "    \n",
    "    }\n",
    "    return(b)\n",
    "}\n",
    "negative_tweets$negative.words <- lapply(negative_tweets$negative.words, negative_words)\n",
    "head(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting our result\n",
    "words.vector<-unlist(negative_tweets$negative.words)\n",
    "freq.list<-table(words.vector)\n",
    "sorted.freq.list<-sort(freq.list, decreasing=TRUE)\n",
    "mfw = head(sorted.freq.list, n = 5)\n",
    "\n",
    "# Simple Bar Plot \n",
    "barplot(mfw, main=\"Top negative (stemmed) words\", xlab=\"negative Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>So, we can see here stemmed words with the highest appearence frequency in negative tweets. We have:\n",
    "    <ol>\n",
    "      <li>'femm', the word with the highest frequency which is not even a real word but regroup words with roots like:\n",
    "        <ol>\n",
    "          <li>femmes</li>\n",
    "          <li>femme</li>\n",
    "        </ol></li>\n",
    "      <li>'flou', that can regroup:\n",
    "        <ol>\n",
    "          <li>flou</li>\n",
    "          <li>flous</li>\n",
    "          <li>And maybe mispelled words like 'floux'</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'lutt' that may regroup:\n",
    "        <ol>\n",
    "            <li>lutte</li>\n",
    "            <li>luttes</li>\n",
    "            <li>lutter, etc.</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'question'</li>\n",
    "      <li>'seul'</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
