{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=6 color=\"#2E1698\"><em><u><center>Data Processing</center></u></em></font></p><br>\n",
    "<p><font size=3.2 color=\"#2E1698\"><i><u>Introduction:</u></i> explain our data blablabla we are here aiming to manipulate the data that we generated before.</font><p><br>\n",
    "\n",
    "<font color=\"#206B50\" size = 3><center>**SUMMARY**</center></font><br>\n",
    "<font size=3.2 color=\"#2E1698\">\n",
    "<ol>\n",
    "      <li>Data cleaning</li>\n",
    "      <li>Provide a list of the 15 most common words</li>\n",
    "      <li>Provide a list of the 2 pairs of words having the highest co-occurrence frequency</li>\n",
    "      <li>Build a graphical representation of the most frequent words with their polarity (pos/neg or anger/joy/fear/...)</li>\n",
    "      <li>Indicate the 3 most frequent representatives words in each category</li>\n",
    "      <li>Compare the results of the two approaches</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART I: DATA CLEANING</B></center></font></p>\n",
    "<font size=3 color=\"#2E1698\">First things first, let's import the csv file. We are using french tweets this is why we need to specify the UTF-8 encoding. To have a better idea of our database we decided to show the first 5 lines.</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = read.csv(\"data/debat_primaire_20000.csv\", encoding=\"UTF-8\")\n",
    "print(dim(tweets))\n",
    "head(tweets, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font color=\"#2E1698\" size = 3.2>As you can see, our data frame contains <font color=\"red\">17</font> columns and <font color=\"red\">20 000</font> rows, let's see the 10 firsts rows.</font><br></p>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see if all the columns have multiple values, or if some are useless.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"UNIQUE\", \"\\n\")\n",
    "cat(\"Favorited: \",head(unique(tweets$favorited)), \"\\n\")\n",
    "cat(\"FavoriteCount: \",head(unique(tweets$favoriteCount)), \"\\n\")\n",
    "cat(\"ReplyToSN: \",head(unique(tweets$replyToSN)), \"\\n\")\n",
    "cat(\"ReplyToUID: \",head(unique(tweets$replyToUID)), \"\\n\")\n",
    "cat(\"Id: \",head(unique(tweets$id)), \"\\n\")\n",
    "cat(\"IsRetweet: \",head(unique(tweets$isRetweet)), \"\\n\")\n",
    "cat(\"Lattitude: \",head(unique(tweets$latitude)), \"\\n\")\n",
    "cat(\"Longitude: \",head(unique(tweets$longitude)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Favorited TRUE: \", length(which(tweets$favorited == \"TRUE\")), \"\\n\")\n",
    "cat(\"Favorited FALSE: \", length(which(tweets$favorited == \"FALSE\")), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font  \n",
    "color=\"#2E1698\" size = 3.2>We can see here that there is no TRUE value for favorited, only FALSE. favorited is useless though.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"No favoritedCount: \", length(which(tweets$favoriteCount == 0)), \"\\n\")\n",
    "cat(\"At least one favoritedCount: \", length(which(tweets$favoriteCount != 0)), \"\\n\")\n",
    "cat(\"Percentage of tweets that have favoritedCount\",(3919/20000)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>The favoriteCount have multiple values, 20% of the are not 0 we better keep this column. It is maybe a significative data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Tweets containing longitude information: \", length(which(tweets$longitude != \"NA\")), \"\\n\")\n",
    "cat(\"Tweets containing latitude information: \", length(which(tweets$latitude != \"NA\")), \"\\n\")\n",
    "cat(\"Percentage of tweets containing infos: \", 9/20000*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>There is only 9 tweets over 20 000 that contains latitude and longitude, this represents only 0.045% of the tweets, this info can be considered as useless, and we can delete this two columns too.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat(\"Tweets with replyToSN not null\", length(which(tweets$replyToSN != \"NA\")),\"\\n\")\n",
    "cat(\"Tweets with replyToUID not null\", length(which(tweets$replyToUID != 'NA')), \"\\n\")\n",
    "cat (\"Tweets with replyToSID not null\", length(which(tweets$replyToSID != 'NA')), \"\\n\")\n",
    "cat(\"Percentage of tweets containing replyToSN/UID\", 698/20000*100, \"%\", \"\\n\")\n",
    "cat(\"Percentage of tweets containing replyToSID\", 445/20000*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>There is only about 3.5% of the replytoSN and replyToUID data that are not NA, we can delete these two columns as they don't seem to be interesting to study. Same thing for the replyToSID, with less than 2.5%. <br><br>\n",
    "Let's delete these useless columns!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets <- subset(tweets, select=-c(replyToSN,replyToUID, replyToSID, latitude, longitude, favorited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "head(tweets, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>If we want to use the text, it have to be cleaned first</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_text = function(x)\n",
    "{\n",
    "    #To convert the text in lowercase\n",
    "    try.error = function(z)\n",
    "    {\n",
    "        y = NA\n",
    "        try_error = tryCatch(tolower(z), error=function(e) e)\n",
    "            if (!inherits(try_error, \"error\"))\n",
    "                y = tolower(z)\n",
    "                return(y)\n",
    "    }\n",
    "            \n",
    "    x = sapply(x, try.error)\n",
    "    \n",
    "    #Keep lepen > 3 letters\n",
    "    x = gsub(\"le pen\", \"lepen\", x)\n",
    "    x = gsub(\"#primaire\\\\w+ *\", \"\", x)\n",
    "            \n",
    "     #remove all links starting by http\n",
    "    x = gsub('http\\\\S+\\\\s*', '', x)\n",
    "            \n",
    "    # replace apostrophes\n",
    "    x = gsub(\"'\", \" \", x)\n",
    "\n",
    "    # remove punctuation except @, #, _, -\n",
    "    x = gsub(\"@\", \"AAAAAAAAAAA\", x)\n",
    "    x = gsub(\"#\", \"BBBBBBBBBBB\", x)\n",
    "    x = gsub(\"_\", \"CCCCCCCCCCC\", x)\n",
    "    x = gsub(\"-\", \"DDDDDDDDDDD\", x)\n",
    "    x = gsub(\"[[:punct:]]\", \" \", x)\n",
    "    x = gsub(\"AAAAAAAAAAA\", \"@\", x)\n",
    "    x = gsub(\"BBBBBBBBBBB\", \"#\", x)\n",
    "    x = gsub(\"CCCCCCCCCCC\", \"_\", x)\n",
    "    x = gsub(\"DDDDDDDDDDD\", \"-\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved @\n",
    "    x = gsub(\"@ \", \"@\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved _\n",
    "    x = gsub(\"_ \", \"_\", x)\n",
    "            \n",
    "    # correcting the spaces after the conserved -\n",
    "    x = gsub(\"- \", \"-\", x)\n",
    "    \n",
    "    # remove numbers/Digits\n",
    "    x = gsub(\"[[:digit:]]\", \"\", x)\n",
    "    \n",
    "    # remove tabs\n",
    "    x = gsub(\"[ |\\t]{2,}\", \" \", x)\n",
    "            \n",
    "    # remove blank spaces at the beginning/end\n",
    "    x = gsub(\"^ \", \"\", x)  \n",
    "    x = gsub(\" $\", \"\", x)\n",
    "    \n",
    "    \n",
    "    # As we have already a column indicating if the tweet is a retweet or not \n",
    "    # we can remove \"RT @xxx\" in the tweet header\n",
    "    x = gsub(\"rt @\\\\w+ *\", \"\", x)\n",
    "    x = gsub('\\\\b\\\\w{1,3}\\\\s','', x)\n",
    "    x = gsub('bachar', '', x)\n",
    "    x = gsub('assad', 'alassad', x)\n",
    "            \n",
    "    # remove double spaces\n",
    "    x = gsub(\"  \", \" \", x)\n",
    "    x = gsub(\"  \", \" \", x)\n",
    "    return(x)\n",
    "}\n",
    "                             \n",
    "tweets$text <- clean_text(tweets$text)\n",
    "head(tweets, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Let's see which are the most used @xxx and replace them with words. Afterward we will delete all the @xxx that will not be replaced</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "col = tweets$text\n",
    "at.pattern = \"@\\\\w+ *\"\n",
    "have.at = grep(x = col, pattern = at.pattern)\n",
    "at.matches = gregexpr(pattern = at.pattern,\n",
    "                        text = col[have.at])\n",
    "extracted.at = regmatches(x = col[have.at], m = at.matches)\n",
    "\n",
    "# most frequent words\n",
    "mfw = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "mfw = gsub(\" \", \"\", mfw)\n",
    "top40_user_called = sort(table(unlist(mfw)), decreasing=TRUE)\n",
    "head(top40_user_called, n = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_name_text = function(x)\n",
    "{\n",
    "  x = gsub('\\\\S+hamo\\\\S+', 'hamon', x)\n",
    "  x = gsub('\\\\S+ontebou\\\\S+', 'montebourg', x)\n",
    "  x = gsub('\\\\S+vall\\\\S+', 'valls', x)\n",
    "  x = gsub('\\\\S+peillon\\\\S+', 'peillon', x)\n",
    "  x = gsub('\\\\S+rugy\\\\S+', 'derugy', x)\n",
    "  x = gsub('\\\\S+macro\\\\S+', 'macron', x)\n",
    "  x = gsub('\\\\S+francet\\\\S+', 'francetv', x)\n",
    "  x = gsub('\\\\S+pinel\\\\S+', 'pinel', x)\n",
    "  x = gsub('\\\\S+nnahmia\\\\S+', 'bennahmias', x)\n",
    "  x = gsub('\\\\S+eunesavecam\\\\S+', 'montebourg', x) #jeunes avec Arnaud Montebourg\n",
    "  x = gsub('\\\\S+galut\\\\S+', 'galut', x) \n",
    "  x = gsub('\\\\S+donald\\\\S+', 'trump', x)\n",
    "  x = gsub('\\\\S+trump\\\\S+', 'trump', x)\n",
    "  x = gsub('\\\\S+najat\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+vallaud\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+elkacem\\\\S+', 'najatvb', x)\n",
    "  x = gsub('\\\\S+europe\\\\S+', 'europe', x)\n",
    "  x = gsub('\\\\S+olland\\\\S+', 'hollande', x)\n",
    "  x = gsub('\\\\S+ujada\\\\S+', 'pujadas', x)\n",
    "  x = gsub('\\\\S+ujada\\\\S+', 'pujadas', x)\n",
    "  x = gsub('\\\\S+taubir\\\\S+', 'taubira', x)\n",
    "  x = gsub('\\\\S+sapin\\\\S+', 'sapin', x)\n",
    "  x = gsub('\\\\S+guillaumetc\\\\S+', 'taubira', x)\n",
    "  x = gsub('\\\\S+aubry\\\\S+', 'aubry', x)\n",
    "  x = gsub('\\\\S+compile\\\\S+', 'compile', x)\n",
    "  x = gsub('\\\\S+melenchon\\\\S+', 'melenchon', x)\n",
    "  x = gsub('\\\\S+francei\\\\S+', 'franceinfo', x)\n",
    "  x = gsub('\\\\S+bfm\\\\S+', 'bfmtv', x)\n",
    "  x = gsub('\\\\S+namia\\\\S+', 'namias', x)\n",
    "  x = gsub('\\\\S+vp_\\\\S+', 'peillon', x)\n",
    "  x = gsub('\\\\S+fillon\\\\S+', 'fillon', x)\n",
    "  x = gsub('\\\\S+avecmv\\\\S+', 'valls', x) #Avec Manuel Valls\n",
    "  x = gsub('\\\\S+mlp\\\\S+', 'lepen', x)\n",
    "\n",
    "  x = gsub(\"#\\\\w+ *\", \"\", x)\n",
    "  x = gsub(\"@\\\\w+ *\", \"\", x)\n",
    "}\n",
    "\n",
    "tweets$text <- clean_name_text(tweets$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART II: LIST OF 15 MOST COMMON WORD</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>Let's see which are the most used words</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "at.pattern = \"[a-zA-Z]\\\\w+ *\"\n",
    "have.at = grep(x = col, pattern = at.pattern)\n",
    "at.matches = gregexpr(pattern = at.pattern,\n",
    "                        text = col[have.at])\n",
    "extracted.at = regmatches(x = col[have.at], m = at.matches)\n",
    "\n",
    "# most frequent words\n",
    "mfw = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "mfw = gsub(\" \", \"\", mfw)\n",
    "w = sort(table(unlist(mfw)), decreasing=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "number_Top = function(number){\n",
    "  topWord = head(w, n = number)\n",
    "  topWord = sort(topWord, decreasing=FALSE) \n",
    "  return(topWord)\n",
    "}\n",
    "\n",
    "top15 = number_Top(15)\n",
    "barplot(top15, border=NA, las=2, main=\"Top 15 most frequent word\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>We can see in the display of the 15 most frequent word that some of them are not revelant like the preposition 'dans' so we want to clean our text again</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_unused_text = function(x){\n",
    "    \n",
    "    x = gsub(\"dans \", \"\", x)\n",
    "    x = gsub(\"pour \", \"\", x)\n",
    "    x = gsub(\"quand \", \"\", x)\n",
    "    x = gsub(\"avec \", \"\", x)\n",
    "    x = gsub(\"mais \", \"\", x)\n",
    "    x = gsub(\"tre \", \"\", x)\n",
    "    x = gsub(\"vous \", \"\", x)\n",
    "    x = gsub(\"nous \", \"\", x)\n",
    "    x = gsub(\"comme \", \"\", x)\n",
    "    x = gsub(\"plus \", \"\", x)\n",
    "    x = gsub(\"tout \", \"\", x)\n",
    "    x = gsub(\"sont \", \"\", x)\n",
    "}\n",
    "\n",
    "\n",
    "tweets$text <- clean_unused_text(tweets$text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "col = tweets$text\n",
    "at.pattern = \"[a-zA-Z]\\\\w+ *\"\n",
    "have.at = grep(x = col, pattern = at.pattern)\n",
    "at.matches = gregexpr(pattern = at.pattern, text = col[have.at])\n",
    "extracted.at = regmatches(x = col[have.at], m = at.matches)\n",
    "\n",
    "# most frequent words\n",
    "mfw = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "mfw = gsub(\" \", \"\", mfw)\n",
    "w = sort(table(unlist(mfw)), decreasing=TRUE)\n",
    "#head(d, n = 20)\n",
    "\n",
    "top15 = number_Top(15)\n",
    "barplot(top15, border=NA, las=2, main=\"Top 15 most frequent word\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"tm\")\n",
    "library(tm) # need R 3.3.2\n",
    "docs <- Corpus(VectorSource(tweets$text))\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "findAssocs(dtm, c(\"montebourg\", \"hamon\", \"valls\", \"macron\", \"gauche\", \"faut\", \"femmes\", \"france\", \"veux\", \n",
    "                  \"europe\", \"peillon\", \"trump\", \"politique\", \"doit\", \"bennahmias\"), corlimit = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART III: GRAPHICAL REPRESENTATION</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>The first graphical representation we'll do is a wordcloud representing what we found in the part above (most used words). To do that we will use wordcloud library which is really representative for our work. Let's see what does it look like in our case.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"wordcloud\")\n",
    "#install.packages(\"SnowballC\")\n",
    "library(SnowballC)\n",
    "library(wordcloud)\n",
    "\n",
    "top50 = number_Top(50)\n",
    "wordcloud(names(top50), top50, min.freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "require(devtools)\n",
    "install_github(\"lchiffon/wordcloud2\")\n",
    "library(wordcloud2)\n",
    "\n",
    "top100 = number_Top(100)\n",
    "cat(\"The letterCloud function does not work on Jupyter notebook, if you are using markdown, you can uncomment this line.\")\n",
    "#letterCloud(top100, word = \"PS\", wordSize = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><font size=3 color=\"#206B50\"><center><B>PART V: Indicate 3 most frequent words in each category</B></center></font></p>\n",
    "<font color=\"#2E1698\" size = 3.2>The first step is to categorize each tweet then we will see the most common word in each category (positive, neutral, negative). So far we have found and traduce a list of positive and negative words, stored into a .txt file.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reminder for later, keep the \"pas\", \"ne\" and all noted negation words as they change the score of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentence <- tweets$text\n",
    "head(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation tweets function\n",
    "#install.packages(\"plyr\")\n",
    "score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')\n",
    "{\n",
    "  require(plyr)\n",
    "  require(stringr)\n",
    "  scores <- laply(sentences, function(sentence, pos.words, neg.words){\n",
    "    word.list <- str_split(sentence, ' ')\n",
    "    words <- unlist(word.list)\n",
    "    pos.matches <- match(words, pos.words)\n",
    "    neg.matches <- match(words, neg.words)\n",
    "    pos.matches <- !is.na(pos.matches)\n",
    "    neg.matches <- !is.na(neg.matches)\n",
    "    score <- sum(pos.matches) - sum(neg.matches)\n",
    "    return(score)\n",
    "  }, pos.words, neg.words, .progress=.progress)\n",
    " scores.df <- data.frame(score=scores, text=sentences)\n",
    " return(scores.df)\n",
    "}\n",
    "pos <- scan('data/NegPos/Positive.txt', what='character', comment.char=';') #folder with positive dictionary\n",
    "neg <- scan('data/NegPos/Negative.txt', what='character', comment.char=';') #folder with negative dictionary\n",
    "scores <- score.sentiment(sentence, pos, neg, .progress='text')\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total evaluation: positive / negative / neutral\n",
    " stat <- scores\n",
    " stat$created <- tweets$created\n",
    " stat$created <- as.Date(stat$created)\n",
    " stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>As you can see here there is a problem, some tweets are not scored as they should be. For example the word \"incarner\" could be in the 'Positive' dictionary but written as \"incarne\" or \"incarné\". We should use stemming both on our tweets and on our positive/negative dictionaries.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Let's see what we get using stemming this time!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First we read the negative dictionary and store it into a variable\n",
    "neg = read.table('data/NegPos/Negative.txt', sep='\\n')\n",
    "# There might be some values present more than once, so let's make it unique\n",
    "neg = unique(neg)\n",
    "neg = neg$V1\n",
    "# To use stemming we have to convert our list of words into a corpus.\n",
    "negative = VCorpus(VectorSource(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Same thing here for the positive dictionary\n",
    "pos = read.table('data/NegPos/Positive.txt', sep='\\n')\n",
    "pos = unique(pos)\n",
    "pos = pos$V1\n",
    "positive = VCorpus(VectorSource(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Stemming of the positive dictionary\n",
    "pos_stem <- tm_map(positive, PlainTextDocument)  # needs to come before stemming\n",
    "pos_stem <- tm_map(pos_stem, stemDocument, \"french\")\n",
    "#Converting the corpus into a dataframe\n",
    "dataframe_pos<-data.frame(text=unlist(sapply(pos_stem, `[`, \"content\")), stringsAsFactors=F)\n",
    "dataframe_pos<-unique(dataframe_pos)\n",
    "\n",
    "#Stemming of the negative dictionary                                                       \n",
    "neg_stem <- tm_map(negative, PlainTextDocument)  # needs to come before stemming\n",
    "neg_stem <- tm_map(neg_stem, stemDocument, \"french\")\n",
    "#Converting the corpus into a dataframe\n",
    "dataframe_neg<-data.frame(text=unlist(sapply(neg_stem, `[`, \"content\")), stringsAsFactors=F)\n",
    "dataframe_neg<-unique(dataframe_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's put the dataframes into a proper format\n",
    "z_pos = c(dataframe_pos$text)\n",
    "z_neg = c(dataframe_neg$text)\n",
    "head(z_pos)\n",
    "head(z_neg)\n",
    "\n",
    "# And now we create 2 txt files containing the words negative and positive after stemming\n",
    "\n",
    "#write.table(z_neg, file = \"NegPos/NegativeStem.txt\", append = FALSE, quote = FALSE, sep = \"\\n\",\n",
    "#            na = \"NA\", dec = \".\", row.names = FALSE,\n",
    "#            col.names = FALSE)\n",
    "#write.table(z_pos, file = \"NegPos/PositiveStem.txt\", append = FALSE, quote = FALSE, sep = \"\\n\",\n",
    "#            na = \"NA\", dec = \".\", row.names = FALSE,\n",
    "#            col.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stemming of the tweets\n",
    "txt <- tweets$text\n",
    "txt = VCorpus(VectorSource(txt))\n",
    "s <- tm_map(txt, PlainTextDocument)  # needs to come before stemming\n",
    "s <- tm_map(s, stemDocument, \"french\")\n",
    "                                                  \n",
    "dataframe_stem<-data.frame(text=unlist(sapply(s, `[`, \"content\")), stringsAsFactors=F)\n",
    "head(dataframe_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#evaluation tweets after stemming function\n",
    "score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')\n",
    "{\n",
    "  require(plyr)\n",
    "  require(stringr)\n",
    "  scores <- laply(sentences, function(sentence, pos.words, neg.words){\n",
    "    word.list <- str_split(sentence, ' ')\n",
    "    words <- unlist(word.list)\n",
    "    pos.matches <- match(words, pos.words)\n",
    "    neg.matches <- match(words, neg.words)\n",
    "    pos.matches <- !is.na(pos.matches)\n",
    "    neg.matches <- !is.na(neg.matches)\n",
    "    score <- sum(pos.matches) - sum(neg.matches)\n",
    "    return(score)\n",
    "  }, pos.words, neg.words, .progress=.progress)\n",
    " scores.df <- data.frame(score=scores, text=sentences)\n",
    " return(scores.df)\n",
    "}\n",
    "pos <- scan(\"data/NegPos/PositiveStem.txt\", what='character', comment.char=';') #folder with positive dictionary\n",
    "neg <- scan(\"data/NegPos/NegativeStem.txt\", what='character', comment.char=';') #folder with negative dictionary\n",
    "head(neg)\n",
    "scores <- score.sentiment(dataframe_stem$text, z_pos, z_neg, .progress='text')\n",
    "head(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#total evaluation: positive / negative / neutral\n",
    " stat <- scores\n",
    " stat$original_tweet <- tweets$text\n",
    " stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n",
    "\n",
    "head(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now that we have a score on each tweet, let's study the words' occurence.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>We are going to simply see, like we did before, which word is appearing the most in our positive/negative only dataframe taht we are going to create.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Building a dataframe with only positive tweets.\n",
    "positive_tweets = subset(stat, tweet == 'positive')\n",
    "head(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's see which are the word that appear the most:\n",
    "tweet = positive_tweets$original_tweet\n",
    "at.pattern = \"[a-zA-Z]\\\\w+ *\"\n",
    "have.at = grep(x = tweet, pattern = at.pattern)\n",
    "at.matches = gregexpr(pattern = at.pattern,\n",
    "                        text = tweet[have.at])\n",
    "extracted.at = regmatches(x = tweet[have.at], m = at.matches)\n",
    "\n",
    "# most frequent words\n",
    "mfw = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "mfw = gsub(\" \", \"\", mfw)\n",
    "w = sort(table(unlist(mfw)), decreasing=TRUE)\n",
    "\n",
    "#Using the function number_top already defined before:\n",
    "top3 = number_Top(3)\n",
    "barplot(top3, border=NA, las=2, main=\"Top 3 most frequent word in positive tweets\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Building a dataframe with only negative tweets.\n",
    "negative_tweets = subset(stat, tweet == 'negative')\n",
    "head(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's see which are the word that appear the most (again):\n",
    "tweet = negative_tweets$original_tweet\n",
    "at.pattern = \"[a-zA-Z]\\\\w+ *\"\n",
    "have.at = grep(x = tweet, pattern = at.pattern)\n",
    "at.matches = gregexpr(pattern = at.pattern,\n",
    "                        text = tweet[have.at])\n",
    "extracted.at = regmatches(x = tweet[have.at], m = at.matches)\n",
    "\n",
    "# most frequent words\n",
    "mfw = sort(unlist(extracted.at), decreasing=TRUE)\n",
    "mfw = gsub(\" \", \"\", mfw)\n",
    "w = sort(table(unlist(mfw)), decreasing=TRUE)\n",
    "\n",
    "#Using the function number_top already defined before:\n",
    "top3 = number_Top(3)\n",
    "barplot(top3, border=NA, las=2, main=\"Top 3 most frequent words in negative tweets\", cex.main=1, horiz=TRUE, col= \"darkblue\", cex.names=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Well, we can see after this little study that there is not much differences between 'positive' and 'negative' tweets. The most occuring words are neither positive or negative and are the same as in the top 15 we did before.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>Now let's see what we get using another strategy:\n",
    "    <ol>\n",
    "      <li>Building a column containing only positive/negative words of a tweet</li>\n",
    "      <li>Analyse this column only and see which words are appearing the most</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Building positive.words column:\n",
    "pos <- scan(\"data/NegPos/PositiveStem.txt\", what='character', comment.char=';')\n",
    "word.list <- str_split(positive_tweets$text, \" \")\n",
    "positive_tweets$positive.words <- word.list\n",
    "\n",
    "# this function will create a list of words by comparing if the words in tweet are in the dictionary\n",
    "positive_words = function(sentence){\n",
    "    b = c()\n",
    "    i = 0\n",
    "    for(w in sentence){\n",
    "        if(w %in% pos){\n",
    "            i = i + 1\n",
    "            b[i] <- w\n",
    "        }\n",
    "    }\n",
    "    return(b)\n",
    "}\n",
    "positive_tweets$positive.words <- lapply(positive_tweets$positive.words, positive_words)\n",
    "head(positive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Plotting our result\n",
    "words.vector<-unlist(positive_tweets$positive.words)\n",
    "freq.list<-table(words.vector)\n",
    "sorted.freq.list<-sort(freq.list, decreasing=TRUE)\n",
    "mfw = head(sorted.freq.list, n = 5)\n",
    "\n",
    "# Simple Bar Plot \n",
    "barplot(mfw, main=\"Top positive (stemmed) words\", \n",
    "  \txlab=\"positive Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>So, we can see here stemmed words with the highest appearence frequency in positive tweets. We have:\n",
    "    <ol>\n",
    "      <li>'fait', the word with the highest frequency.</li>\n",
    "      <li>'droit', with stemming it regroups also words of the same root, like: \n",
    "        <ol>\n",
    "          <li>droits</li>\n",
    "          <li>droite</li>\n",
    "          <li>droites</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'bien'</li>\n",
    "      <li>'sérieux'</li>\n",
    "      <li>'légitim', which is not even a real word but with stemming it regroup words of the same root like:\n",
    "        <ol>\n",
    "          <li>légitime</li>\n",
    "          <li>légitimes</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Building negative.words column:\n",
    "neg <- scan(\"data/NegPos/NegativeStem.txt\", what='character', comment.char=';')\n",
    "word.list <- str_split(negative_tweets$text, \" \")\n",
    "negative_tweets$negative.words <- word.list\n",
    "\n",
    "# this function will create a list of words by comparing if the words in tweet are in the dictionary\n",
    "negative_words = function(sentence){\n",
    "    b = c()\n",
    "    i = 0\n",
    "    for(w in sentence){\n",
    "        if(w %in% neg){\n",
    "            i = i + 1\n",
    "            b[i] <- w\n",
    "        }\n",
    "    \n",
    "    }\n",
    "    return(b)\n",
    "}\n",
    "negative_tweets$negative.words <- lapply(negative_tweets$negative.words, negative_words)\n",
    "head(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Plotting our result\n",
    "words.vector<-unlist(negative_tweets$negative.words)\n",
    "freq.list<-table(words.vector)\n",
    "sorted.freq.list<-sort(freq.list, decreasing=TRUE)\n",
    "mfw = head(sorted.freq.list, n = 5)\n",
    "\n",
    "# Simple Bar Plot \n",
    "barplot(mfw, main=\"Top negative (stemmed) words\", \n",
    "  \txlab=\"negative Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font color=\"#2E1698\" size = 3.2>So, we can see here stemmed words with the highest appearence frequency in negative tweets. We have:\n",
    "    <ol>\n",
    "      <li>'femm', the word with the highest frequency which is not even a real word but regroup words with roots like:\n",
    "        <ol>\n",
    "          <li>femmes</li>\n",
    "          <li>femme</li>\n",
    "        </ol></li>\n",
    "      <li>'faut', with stemming it regroups also words of the same root, like: \n",
    "        <ol>\n",
    "          <li>faut</li>\n",
    "          <li>faute</li>\n",
    "          <li>fautes</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'flou', that can regroup:\n",
    "        <ol>\n",
    "          <li>flou</li>\n",
    "          <li>flous</li>\n",
    "          <li>And maybe mispelled words like 'floux'</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'lutt' that may regroup:\n",
    "        <ol>\n",
    "            <li>lutte</li>\n",
    "            <li>luttes</li>\n",
    "            <li>lutter, etc.</li>\n",
    "        </ol>\n",
    "      </li>\n",
    "      <li>'seul'</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
